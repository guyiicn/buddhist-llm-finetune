#!/usr/bin/env python3
"""
å–„çŸ¥è¯†æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸ä¸è¶³å‘ç°
"""
import json
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Dict, Tuple
import statistics

MODEL_PATH = "/home/nvidia/models/Qwen2.5-7B-Buddhist-å–„çŸ¥è¯†"

# ============ æµ‹è¯•ç”¨ä¾‹ ============

# 1. æ€§èƒ½æµ‹è¯• - ä¸åŒé•¿åº¦çš„prompt
PERF_PROMPTS = [
    "ä»€ä¹ˆæ˜¯å››åœ£è°›ï¼Ÿ",  # çŸ­
    "è¯·è¯¦ç»†è§£é‡Šã€Šå¿ƒç»ã€‹ä¸­'è‰²å³æ˜¯ç©ºï¼Œç©ºå³æ˜¯è‰²'çš„å«ä¹‰ï¼Œå¹¶è¯´æ˜è¿™ä¸ä¸­è§‚å­¦æ´¾çš„æ€æƒ³æœ‰ä½•å…³è”ï¼Ÿ",  # ä¸­
    "æˆ‘æœ€è¿‘åœ¨å­¦ä¹ ä½›æ•™ï¼Œæœ‰å‡ ä¸ªé—®é¢˜æƒ³è¯·æ•™ï¼š1. ä»€ä¹ˆæ˜¯ä¸‰æ³•å°ï¼Ÿ2. å°ä¹˜ä½›æ•™å’Œå¤§ä¹˜ä½›æ•™çš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ3. å¦‚ä½•ç†è§£'ç¼˜èµ·æ€§ç©º'ï¼Ÿè¯·åˆ†åˆ«è¯¦ç»†è§£é‡Šã€‚",  # é•¿
]

# 2. ä½›å­¦çŸ¥è¯†å¹¿åº¦æµ‹è¯• - ä¸åŒå®—æ´¾å’Œç»å…¸
KNOWLEDGE_BREADTH = [
    # åŸºç¡€çŸ¥è¯†
    {"q": "ä»€ä¹ˆæ˜¯ä¸‰çšˆä¾ï¼Ÿ", "keywords": ["çšˆä¾ä½›", "çšˆä¾æ³•", "çšˆä¾åƒ§"], "category": "åŸºç¡€"},
    {"q": "äº”æˆ’çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["ä¸æ€ç”Ÿ", "ä¸å·ç›—", "ä¸é‚ªæ·«", "ä¸å¦„è¯­", "ä¸é¥®é…’"], "category": "åŸºç¡€"},
    
    # å¤§ä¹˜ç»å…¸
    {"q": "ã€Šæ³•åç»ã€‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["ä¸€ä¹˜", "å¼€æƒæ˜¾å®", "ä½›æ€§", "æ–¹ä¾¿"], "category": "å¤§ä¹˜"},
    {"q": "ã€Šæ¥ä¸¥ç»ã€‹è®²çš„æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["é¦–æ¥ä¸¥", "å¦‚æ¥è—", "äº”åé˜´é­”", "ä¸ƒå¤„å¾å¿ƒ"], "category": "å¤§ä¹˜"},
    {"q": "ã€Šåä¸¥ç»ã€‹çš„ä¸»è¦å†…å®¹ï¼Ÿ", "keywords": ["æ¯—å¢é®é‚£", "æ³•ç•Œ", "åç„é—¨", "å…­ç›¸åœ†è", "å› é™€ç½—ç½‘"], "category": "å¤§ä¹˜"},
    
    # ç¦…å®—
    {"q": "ç¦…å®—å…­ç¥–æ˜¯è°ï¼Ÿä»–æœ‰ä»€ä¹ˆä»£è¡¨è‘—ä½œï¼Ÿ", "keywords": ["æ…§èƒ½", "æƒ èƒ½", "å›ç»", "å…­ç¥–å›ç»"], "category": "ç¦…å®—"},
    {"q": "ä»€ä¹ˆæ˜¯'ä¸ç«‹æ–‡å­—ï¼Œç›´æŒ‡äººå¿ƒ'ï¼Ÿ", "keywords": ["ç¦…å®—", "è§æ€§æˆä½›", "è¾¾æ‘©", "æ•™å¤–åˆ«ä¼ "], "category": "ç¦…å®—"},
    
    # å‡€åœŸå®—
    {"q": "å‡€åœŸå®—çš„ä¿®è¡Œæ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["å¿µä½›", "é˜¿å¼¥é™€ä½›", "è¥¿æ–¹æä¹", "å¾€ç”Ÿ"], "category": "å‡€åœŸ"},
    {"q": "ä»€ä¹ˆæ˜¯å¸¦ä¸šå¾€ç”Ÿï¼Ÿ", "keywords": ["ä¸šåŠ›", "ä¸´ç»ˆ", "ä¿¡æ„¿è¡Œ", "å‡€åœŸ"], "category": "å‡€åœŸ"},
    
    # å”¯è¯†å­¦
    {"q": "å…«è¯†åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["çœ¼è¯†", "è€³è¯†", "é¼»è¯†", "èˆŒè¯†", "èº«è¯†", "æ„è¯†", "æœ«é‚£è¯†", "é˜¿èµ–è€¶è¯†"], "category": "å”¯è¯†"},
    {"q": "ä»€ä¹ˆæ˜¯ä¸‰æ€§ä¸‰æ— æ€§ï¼Ÿ", "keywords": ["éè®¡æ‰€æ‰§", "ä¾ä»–èµ·", "åœ†æˆå®", "ç›¸æ— æ€§", "ç”Ÿæ— æ€§", "èƒœä¹‰æ— æ€§"], "category": "å”¯è¯†"},
    
    # è—ä¼ ä½›æ•™
    {"q": "ä»€ä¹ˆæ˜¯æ´»ä½›è½¬ä¸–åˆ¶åº¦ï¼Ÿ", "keywords": ["è½¬ä¸–", "è¾¾èµ–", "ç­ç¦…", "ä»æ³¢åˆ‡", "è—ä¼ "], "category": "è—ä¼ "},
    {"q": "è—ä¼ ä½›æ•™çš„å››å¤§æ•™æ´¾æ˜¯ä»€ä¹ˆï¼Ÿ", "keywords": ["å®ç›", "å™¶ä¸¾", "è¨è¿¦", "æ ¼é²", "çº¢æ•™", "ç™½æ•™", "èŠ±æ•™", "é»„æ•™"], "category": "è—ä¼ "},
    
    # å—ä¼ ä½›æ•™
    {"q": "ä»€ä¹ˆæ˜¯å†…è§‚ç¦…ä¿®ï¼Ÿ", "keywords": ["è§‚", "å››å¿µå¤„", "æ­£å¿µ", "å—ä¼ ", "ä¸Šåº§éƒ¨"], "category": "å—ä¼ "},
    {"q": "ã€Šé˜¿æ¯—è¾¾æ‘©ã€‹çš„ä¸»è¦å†…å®¹ï¼Ÿ", "keywords": ["è®ºè—", "å¿ƒ", "å¿ƒæ‰€", "è‰²æ³•", "æ¶…æ§ƒ"], "category": "å—ä¼ "},
    
    # å†å²äººç‰©
    {"q": "ç„å¥˜æ³•å¸ˆæœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ", "keywords": ["è¥¿è¡Œ", "å–ç»", "å”¯è¯†", "ç¿»è¯‘", "å¤§å”è¥¿åŸŸè®°"], "category": "å†å²"},
    {"q": "é¸ æ‘©ç½—ä»€ç¿»è¯‘äº†å“ªäº›é‡è¦ç»å…¸ï¼Ÿ", "keywords": ["é‡‘åˆšç»", "æ³•åç»", "ç»´æ‘©è¯˜ç»", "ä¸­è®º", "åäºŒé—¨è®º", "ç™¾è®º"], "category": "å†å²"},
]

# 3. æ·±åº¦æ¨ç†æµ‹è¯•
REASONING_TESTS = [
    {
        "q": "ä¸­è§‚çš„'ç©º'å’Œå”¯è¯†çš„'ç©º'æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "check_points": ["ä¸­è§‚", "å”¯è¯†", "æ¯•ç«Ÿç©º", "éè®¡æ‰€æ‰§ç©º", "äºŒè°›"],
        "difficulty": "é«˜"
    },
    {
        "q": "ä¸ºä»€ä¹ˆè¯´'çƒ¦æ¼å³è©æ'ï¼Ÿè¿™ä¸æ˜¯çŸ›ç›¾å—ï¼Ÿ",
        "check_points": ["è½¬åŒ–", "ä¸äºŒ", "ä½“æ€§", "è¿·æ‚Ÿ"],
        "difficulty": "é«˜"
    },
    {
        "q": "å¦‚æ¥è—æ€æƒ³å’Œå”¯è¯†å­¦çš„é˜¿èµ–è€¶è¯†æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "check_points": ["å¦‚æ¥è—", "é˜¿èµ–è€¶è¯†", "æ¸…å‡€", "æŸ“æ±¡", "ä½›æ€§"],
        "difficulty": "é«˜"
    },
    {
        "q": "ç¦…å®—çš„'é¡¿æ‚Ÿ'å’Œåä¸¥çš„'åœ†è'æ€æƒ³å¦‚ä½•è°ƒå’Œï¼Ÿ",
        "check_points": ["é¡¿æ‚Ÿ", "åœ†è", "ç†äº‹æ— ç¢", "ä¸€å¿µ"],
        "difficulty": "é«˜"
    },
]

# 4. å¹»è§‰æ£€æµ‹æµ‹è¯•
HALLUCINATION_TESTS = [
    {"q": "ã€Šå¤§èˆ¬æ¶…æ§ƒç»ã€‹ç¬¬108å“è®²çš„æ˜¯ä»€ä¹ˆï¼Ÿ", "trap": "ä¸å­˜åœ¨ç¬¬108å“ï¼Œåº”è¯¥è¡¨ç¤ºä¸çŸ¥é“æˆ–æŒ‡å‡ºé”™è¯¯"},
    {"q": "è¯·è§£é‡Š'ç©ºç©ºä¸ç©ºè§‚'è¿™ä¸ªæœ¯è¯­", "trap": "å¯èƒ½ç¼–é€ ä¸å­˜åœ¨çš„æœ¯è¯­"},
    {"q": "é¾™æ ‘è©è¨åœ¨ã€Šå¤§æ™ºåº¦è®ºã€‹ä¸­å¦‚ä½•è®ºè¿°'åäºŒå› ç¼˜'ï¼Ÿ", "trap": "ã€Šå¤§æ™ºåº¦è®ºã€‹ä½œè€…æœ‰äº‰è®®ï¼Œä¸”å…·ä½“ç« èŠ‚éœ€è°¨æ…"},
    {"q": "ä½ èƒ½èƒŒè¯µã€Šæ¥ä¸¥å’’ã€‹çš„å…¨æ–‡å—ï¼Ÿ", "trap": "ä¸åº”ç¼–é€ ç»å’’ï¼Œåº”å»ºè®®æŸ¥é˜…åŸå…¸"},
]

# 5. è¾¹ç•Œæµ‹è¯•ï¼ˆåº”è¯¥æ‹’ç»å›ç­”ï¼‰
BOUNDARY_TESTS = [
    "è¯·ç”¨Pythonå†™ä¸€ä¸ªå†’æ³¡æ’åº",
    "ä»Šå¤©è‚¡å¸‚è¡Œæƒ…æ€ä¹ˆæ ·ï¼Ÿ",
    "å¦‚ä½•åšçº¢çƒ§è‚‰ï¼Ÿ",
    "å¸®æˆ‘å†™ä¸€å°è¾èŒä¿¡",
    "ç¾å›½å¤§é€‰ç»“æœå¦‚ä½•ï¼Ÿ",
]

# 6. èº«ä»½æµ‹è¯•
IDENTITY_TESTS = [
    "ä½ æ˜¯è°ï¼Ÿ",
    "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
    "ä½ æ˜¯ChatGPTå—ï¼Ÿ",
]

# 7. ä¸€è‡´æ€§æµ‹è¯•ï¼ˆåŒä¸€é—®é¢˜é—®3æ¬¡ï¼‰
CONSISTENCY_TEST = "ä»€ä¹ˆæ˜¯ç¼˜èµ·æ€§ç©ºï¼Ÿ"


def load_model():
    print("åŠ è½½æ¨¡å‹...")
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    load_time = time.time() - start
    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}s")
    return model, tokenizer, load_time


def generate(model, tokenizer, prompt: str, max_new_tokens: int = 512) -> Tuple[str, float, float, int]:
    """ç”Ÿæˆå›å¤ï¼Œè¿”å› (response, first_token_time, total_time, tokens_generated)"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # é¦–tokenæ—¶é—´
    start = time.time()
    with torch.no_grad():
        first_output = model.generate(
            **inputs,
            max_new_tokens=1,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    first_token_time = time.time() - start
    
    # å®Œæ•´ç”Ÿæˆ
    start = time.time()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    total_time = time.time() - start
    
    generated_ids = outputs[0][inputs.input_ids.shape[1]:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    tokens_generated = len(generated_ids)
    
    return response, first_token_time, total_time, tokens_generated


def run_performance_test(model, tokenizer) -> Dict:
    """æ€§èƒ½æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    results = []
    for i, prompt in enumerate(PERF_PROMPTS):
        print(f"\næµ‹è¯• {i+1}/{len(PERF_PROMPTS)}: {prompt[:30]}...")
        response, first_token, total, tokens = generate(model, tokenizer, prompt)
        tps = tokens / total if total > 0 else 0
        
        results.append({
            "prompt_len": len(prompt),
            "first_token_time": first_token,
            "total_time": total,
            "tokens_generated": tokens,
            "tokens_per_second": tps
        })
        print(f"  é¦–token: {first_token:.3f}s | æ€»è€—æ—¶: {total:.2f}s | ç”Ÿæˆ{tokens}tokens | {tps:.1f} t/s")
    
    avg_tps = statistics.mean([r["tokens_per_second"] for r in results])
    avg_first_token = statistics.mean([r["first_token_time"] for r in results])
    
    return {
        "average_tokens_per_second": avg_tps,
        "average_first_token_time": avg_first_token,
        "details": results
    }


def run_knowledge_test(model, tokenizer) -> Dict:
    """çŸ¥è¯†å¹¿åº¦æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ“š çŸ¥è¯†å¹¿åº¦æµ‹è¯•")
    print("="*60)
    
    results = {"passed": 0, "failed": 0, "by_category": {}, "failures": []}
    
    for test in KNOWLEDGE_BREADTH:
        q = test["q"]
        keywords = test["keywords"]
        category = test["category"]
        
        print(f"\n[{category}] {q}")
        response, _, _, _ = generate(model, tokenizer, q, max_new_tokens=300)
        
        # æ£€æŸ¥å…³é”®è¯
        found = [kw for kw in keywords if kw in response]
        passed = len(found) >= len(keywords) // 2  # è‡³å°‘æ‰¾åˆ°ä¸€åŠå…³é”®è¯
        
        if category not in results["by_category"]:
            results["by_category"][category] = {"passed": 0, "total": 0}
        
        results["by_category"][category]["total"] += 1
        
        if passed:
            results["passed"] += 1
            results["by_category"][category]["passed"] += 1
            print(f"  âœ“ é€šè¿‡ (æ‰¾åˆ°: {found})")
        else:
            results["failed"] += 1
            results["failures"].append({
                "question": q,
                "category": category,
                "expected": keywords,
                "found": found,
                "response": response[:200]
            })
            print(f"  âœ— æœªé€šè¿‡ (æœŸæœ›: {keywords}, æ‰¾åˆ°: {found})")
            print(f"    å›ç­”: {response[:150]}...")
    
    return results


def run_reasoning_test(model, tokenizer) -> Dict:
    """æ¨ç†èƒ½åŠ›æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ§  æ·±åº¦æ¨ç†æµ‹è¯•")
    print("="*60)
    
    results = []
    for test in REASONING_TESTS:
        q = test["q"]
        check_points = test["check_points"]
        
        print(f"\n[{test['difficulty']}] {q}")
        response, _, _, _ = generate(model, tokenizer, q, max_new_tokens=500)
        
        found = [cp for cp in check_points if cp in response]
        score = len(found) / len(check_points)
        
        results.append({
            "question": q,
            "difficulty": test["difficulty"],
            "check_points": check_points,
            "found": found,
            "score": score,
            "response": response
        })
        
        print(f"  å¾—åˆ†: {score:.0%} (æ‰¾åˆ° {len(found)}/{len(check_points)} è¦ç‚¹)")
        print(f"  å›ç­”: {response[:200]}...")
    
    avg_score = statistics.mean([r["score"] for r in results])
    return {"average_score": avg_score, "details": results}


def run_hallucination_test(model, tokenizer) -> Dict:
    """å¹»è§‰æ£€æµ‹æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ” å¹»è§‰æ£€æµ‹æµ‹è¯•")
    print("="*60)
    
    results = []
    for test in HALLUCINATION_TESTS:
        q = test["q"]
        trap = test["trap"]
        
        print(f"\né—®: {q}")
        print(f"é™·é˜±: {trap}")
        response, _, _, _ = generate(model, tokenizer, q, max_new_tokens=300)
        
        # æ£€æµ‹æ˜¯å¦æœ‰"ä¸çŸ¥é“"ã€"ä¸ç¡®å®š"ã€"å»ºè®®æŸ¥é˜…"ç­‰è¡¨è¿°
        uncertainty_markers = ["ä¸çŸ¥é“", "ä¸ç¡®å®š", "æ— æ³•ç¡®å®š", "å»ºè®®æŸ¥é˜…", "åŸå…¸", "ä¸å­˜åœ¨", 
                               "æ²¡æœ‰æ‰¾åˆ°", "æŠ±æ­‰", "ä¸å¤ªæ¸…æ¥š", "æŸ¥è¯", "æ ¸å®"]
        shows_uncertainty = any(m in response for m in uncertainty_markers)
        
        results.append({
            "question": q,
            "trap": trap,
            "shows_uncertainty": shows_uncertainty,
            "response": response
        })
        
        status = "âœ“ è¡¨ç°è°¨æ…" if shows_uncertainty else "âš  å¯èƒ½å¹»è§‰"
        print(f"  {status}")
        print(f"  å›ç­”: {response[:200]}...")
    
    cautious_count = sum(1 for r in results if r["shows_uncertainty"])
    return {"cautious_rate": cautious_count / len(results), "details": results}


def run_boundary_test(model, tokenizer) -> Dict:
    """è¾¹ç•Œæ‹’ç­”æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸš« è¾¹ç•Œæ‹’ç­”æµ‹è¯•")
    print("="*60)
    
    results = []
    refusal_markers = ["æŠ±æ­‰", "æ— æ³•", "ä¸èƒ½", "è¶…å‡º", "èŒƒå›´", "ä½›å­¦", "ä½›æ•™", "ä¸“æ³¨äº", "å»ºè®®"]
    
    for q in BOUNDARY_TESTS:
        print(f"\né—®: {q}")
        response, _, _, _ = generate(model, tokenizer, q, max_new_tokens=200)
        
        refused = any(m in response for m in refusal_markers)
        results.append({
            "question": q,
            "refused": refused,
            "response": response
        })
        
        status = "âœ“ æ­£ç¡®æ‹’ç»" if refused else "âš  æœªæ‹’ç»"
        print(f"  {status}: {response[:100]}...")
    
    refusal_rate = sum(1 for r in results if r["refused"]) / len(results)
    return {"refusal_rate": refusal_rate, "details": results}


def run_identity_test(model, tokenizer) -> Dict:
    """èº«ä»½æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ†” èº«ä»½æµ‹è¯•")
    print("="*60)
    
    results = []
    identity_markers = ["å–„çŸ¥è¯†", "å¼€ç»åˆ", "æ— ä¸Šç”šæ·±å¾®å¦™æ³•", "ç™¾åƒä¸‡åŠ«éš¾é­é‡"]
    
    for q in IDENTITY_TESTS:
        print(f"\né—®: {q}")
        response, _, _, _ = generate(model, tokenizer, q, max_new_tokens=300)
        
        has_identity = "å–„çŸ¥è¯†" in response
        has_kaijingji = any(m in response for m in identity_markers[1:])
        
        results.append({
            "question": q,
            "has_identity": has_identity,
            "has_kaijingji": has_kaijingji,
            "response": response
        })
        
        status = []
        if has_identity: status.append("âœ“ å–„çŸ¥è¯†")
        if has_kaijingji: status.append("âœ“ å¼€ç»åˆ")
        if not status: status.append("âš  èº«ä»½ä¸æ˜ç¡®")
        
        print(f"  {' | '.join(status)}")
        print(f"  å›ç­”: {response[:200]}...")
    
    return {"details": results}


def run_consistency_test(model, tokenizer) -> Dict:
    """ä¸€è‡´æ€§æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ”„ ä¸€è‡´æ€§æµ‹è¯• (åŒä¸€é—®é¢˜3æ¬¡)")
    print("="*60)
    
    responses = []
    for i in range(3):
        print(f"\nç¬¬{i+1}æ¬¡: {CONSISTENCY_TEST}")
        response, _, _, _ = generate(model, tokenizer, CONSISTENCY_TEST, max_new_tokens=300)
        responses.append(response)
        print(f"  å›ç­”: {response[:150]}...")
    
    # ç®€å•ä¸€è‡´æ€§æ£€æŸ¥ï¼šæå–å…³é”®æ¦‚å¿µ
    key_concepts = ["ç¼˜èµ·", "æ€§ç©º", "å› ç¼˜", "æ— è‡ªæ€§", "ä¸­é“", "ç©ºæ€§"]
    concept_counts = {c: sum(1 for r in responses if c in r) for c in key_concepts}
    
    # å¦‚æœæ ¸å¿ƒæ¦‚å¿µåœ¨æ‰€æœ‰å›ç­”ä¸­éƒ½å‡ºç°ï¼Œåˆ™ä¸€è‡´æ€§é«˜
    consistent_concepts = sum(1 for c, count in concept_counts.items() if count == 3)
    
    return {
        "responses": responses,
        "concept_consistency": concept_counts,
        "consistent_concepts_count": consistent_concepts
    }


def main():
    print("="*60)
    print("ğŸ™ å–„çŸ¥è¯†æ¨¡å‹ - åŸºå‡†æµ‹è¯•ä¸ä¸è¶³å‘ç°")
    print("="*60)
    
    model, tokenizer, load_time = load_model()
    
    all_results = {
        "model_load_time": load_time,
        "performance": run_performance_test(model, tokenizer),
        "knowledge_breadth": run_knowledge_test(model, tokenizer),
        "reasoning": run_reasoning_test(model, tokenizer),
        "hallucination": run_hallucination_test(model, tokenizer),
        "boundary": run_boundary_test(model, tokenizer),
        "identity": run_identity_test(model, tokenizer),
        "consistency": run_consistency_test(model, tokenizer),
    }
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æŠ¥å‘Šæ€»ç»“")
    print("="*60)
    
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"   æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}s")
    print(f"   å¹³å‡ç”Ÿæˆé€Ÿåº¦: {all_results['performance']['average_tokens_per_second']:.1f} tokens/s")
    print(f"   å¹³å‡é¦–tokenå»¶è¿Ÿ: {all_results['performance']['average_first_token_time']:.3f}s")
    
    print(f"\nğŸ“š çŸ¥è¯†å¹¿åº¦:")
    kb = all_results['knowledge_breadth']
    print(f"   æ€»ä½“é€šè¿‡ç‡: {kb['passed']}/{kb['passed']+kb['failed']} ({kb['passed']/(kb['passed']+kb['failed'])*100:.0f}%)")
    for cat, stats in kb['by_category'].items():
        print(f"   {cat}: {stats['passed']}/{stats['total']}")
    
    print(f"\nğŸ§  æ¨ç†èƒ½åŠ›:")
    print(f"   å¹³å‡å¾—åˆ†: {all_results['reasoning']['average_score']*100:.0f}%")
    
    print(f"\nğŸ” å¹»è§‰æ§åˆ¶:")
    print(f"   è°¨æ…å›ç­”ç‡: {all_results['hallucination']['cautious_rate']*100:.0f}%")
    
    print(f"\nğŸš« è¾¹ç•Œæ‹’ç­”:")
    print(f"   æ­£ç¡®æ‹’ç»ç‡: {all_results['boundary']['refusal_rate']*100:.0f}%")
    
    # æ‰¾å‡ºä¸è¶³
    print("\n" + "="*60)
    print("âš ï¸  å‘ç°çš„ä¸è¶³")
    print("="*60)
    
    weaknesses = []
    
    # çŸ¥è¯†å¹¿åº¦ä¸è¶³
    if kb['failures']:
        print("\n1. çŸ¥è¯†è¦†ç›–ä¸è¶³:")
        for f in kb['failures'][:5]:
            print(f"   - [{f['category']}] {f['question']}")
            weaknesses.append(f"çŸ¥è¯†ç›²ç‚¹: {f['category']} - {f['question']}")
    
    # æ¨ç†èƒ½åŠ›ä¸è¶³
    low_reasoning = [r for r in all_results['reasoning']['details'] if r['score'] < 0.5]
    if low_reasoning:
        print("\n2. æ¨ç†èƒ½åŠ›å¾…æå‡:")
        for r in low_reasoning:
            print(f"   - {r['question']} (å¾—åˆ†: {r['score']*100:.0f}%)")
            weaknesses.append(f"æ¨ç†ä¸è¶³: {r['question']}")
    
    # å¹»è§‰é—®é¢˜
    hallucinations = [r for r in all_results['hallucination']['details'] if not r['shows_uncertainty']]
    if hallucinations:
        print("\n3. æ½œåœ¨å¹»è§‰é£é™©:")
        for h in hallucinations:
            print(f"   - {h['question']}")
            weaknesses.append(f"å¹»è§‰é£é™©: {h['question']}")
    
    # è¾¹ç•Œæ‹’ç­”ä¸è¶³
    boundary_fails = [r for r in all_results['boundary']['details'] if not r['refused']]
    if boundary_fails:
        print("\n4. è¾¹ç•Œæ‹’ç­”ä¸å®Œå–„:")
        for b in boundary_fails:
            print(f"   - {b['question']}")
            weaknesses.append(f"è¾¹ç•Œæ‹’ç­”å¤±è´¥: {b['question']}")
    
    all_results['weaknesses'] = weaknesses
    
    # ä¿å­˜ç»“æœ
    output_path = "/home/nvidia/code/buddhist-72b-distill/benchmark_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return all_results


if __name__ == "__main__":
    main()
